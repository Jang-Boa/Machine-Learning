{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-offer",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install numpy pandas matplotlib sklearn xgboost lightgbm catboost \n",
    "# !pip freeze > requirements.txt\n",
    "# !pip install xgboost lightgbm catboost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-attraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Build Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# Imputation and Scaling\"\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, Normalizer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, LabelEncoder\n",
    "\n",
    "# Feature Selection \n",
    "from sklearn.feature_selection import chi2, f_classif, mutual_info_classif\n",
    "from sklearn.feature_selection import SelectKBest, SelectFromModel, SelectPercentile, RFE\n",
    "\n",
    "# Feature Decomposition and Extraction\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Model \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Results\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "\n",
    "# GridSearchCV \n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Save model weights ---- > pickle package or joblib package\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suburban-sense",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-geometry",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('PF_AI_Internal.csv', encoding='utf-8', low_memory=False)\n",
    "# df = pd.read_csv('test_01.csv', encoding='utf-8', low_memory=False)# , dtype={'Gender':object, 'hx_cancer':object, 'Pf_Color_bloody':object})\n",
    "# low_memory옵션은 대용량의 데이터를 불러오는 경우 각 칼럼의 데이터 타입(dtype)을 추측하는 것이 매우 많은 메모리를 사용하기 때문에 대용량의 데이터를 불러올때 메모리 에러가 발생하는 경우 이를 False로 설정할 것을 권장한다.\n",
    "external_df = pd.read_csv('PF_AI_External.csv', encoding='utf-8', low_memory=False)\n",
    "# external_df = pd.read_csv('test_external_01.csv', encoding='utf-8', low_memory=False)\n",
    "\n",
    "clinical_feature = ['Gender', 'Age', 'BMI', 'BT', 'hx_cancer']\n",
    "blood_serum_feature = []\n",
    "pleural_fluid_feature = []\n",
    "for c in df.columns:\n",
    "    if ('b_' in c) & ('_date' not in c):\n",
    "        blood_serum_feature.append(c)\n",
    "    if ('pf_' in c) | ('Pf_' in c):\n",
    "        pleural_fluid_feature.append(c)\n",
    "        \n",
    "print(f'Clinical: {len(clinical_feature)}\\nBlood/Serum: {len(blood_serum_feature)}\\nPleural Fluid: {len(pleural_fluid_feature)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-concord",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" SELECT CATEGORICAL FEATURES AND NUMERICAL FEATURES \"\"\"\n",
    "test_type = 'post_test' # 'pre_test' 'post_test'\n",
    "if test_type == 'pre_test':\n",
    "    # ## Pre-test ## \n",
    "    categorical_features = ['Gender', 'hx_cancer']\n",
    "    numerical_features = ['Age','BMI','BT']\n",
    "\n",
    "elif test_type == 'post_test':\n",
    "## Post-test ##\n",
    "    categorical_features = ['Gender', 'hx_cancer', 'Pf_Color_bloody']\n",
    "    numerical_features = ['Age','BMI','BT',\n",
    "                'pf_pH','pf_RBC','pf_WBC','pf_PMN_p','pf_Lympho_p','pf_other_p',\n",
    "                'pf_protein','pf_glucose','pf_chloride','pf_LD','pf_amylase',\n",
    "                'pf_albumin','pf_ADA','pf_CEA'] + blood_serum_feature\n",
    "print(f'Categorical Data: {len(categorical_features)}\\nNumerical Data: {len(numerical_features)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e00e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical_features = ['hx_cancer']\n",
    "# numerical_features = ['BT', 'pf_pH', 'pf_RBC', 'pf_WBC', 'pf_PMN_p', 'pf_Lympho_p', 'pf_other_p', 'pf_protein', 'pf_LD', 'pf_amylase', 'pf_albumin', 'pf_ADA', 'pf_CEA', 'b_Lympho', 'b_plt', 'b_CRP', 'b_Protein', 'b_TB', 'b_BUN', 'b_Calcium', 'b_Cholesterol', 'b_Creatinine', 'b_Uric_Acid', 'b_albumin']\n",
    "# print(f'Categorical Data: {len(categorical_features)}\\nNumerical Data: {len(numerical_features)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1551e4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical_features = ['hx_cancer']\n",
    "# numerical_features = ['BT', 'pf_pH', 'pf_RBC', 'pf_WBC', 'pf_PMN_p', 'pf_Lympho_p', 'pf_other_p', 'pf_protein', 'pf_LD', 'pf_amylase', 'pf_albumin', 'pf_ADA', 'pf_CEA', 'b_WBC', 'b_Lympho', 'b_plt', 'b_CRP', 'b_TB', 'b_Calcium', 'b_Cholesterol', 'b_total_CO2', 'b_GGT', 'b_K', 'b_Uric_Acid']\n",
    "# print(f'Categorical Data: {len(categorical_features)}\\nNumerical Data: {len(numerical_features)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polished-vienna",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Setting Variables \"\"\"\n",
    "random_state = 42\n",
    "\n",
    "fontsize = 15\n",
    "title_font = 16\n",
    "small_font = 12\n",
    "mean_color = '#1a50b6'\n",
    "data_dir = 'external' \n",
    "\n",
    "label_dict = {0:'Transudative',1:'Malignant',2:'Parapneumonic',3:'Tuberculous',4:'Others'}\n",
    "\n",
    "X_features = categorical_features + numerical_features # clinical_feature + blood_serum_feature + pleural_fluid_feature\n",
    "Y_target = 'final_dx_1'\n",
    "print(f'Number of Features Uses: {len(X_features)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpful-influence",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exempt-membership",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Set X(Data) and Y(Target) \"\"\"\n",
    "X = df.loc[:, X_features]\n",
    "feature_col = X.columns\n",
    "Y = df.loc[:, Y_target]\n",
    "print(f\"Feature: {X.shape} | Target: {Y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-token",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TRAIN AND TEST DATASET SPLIT \"\"\"\n",
    "test_size = 0.2\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, \n",
    "                                                    test_size=test_size, shuffle=True, \n",
    "                                                    stratify=Y, random_state=random_state)\n",
    "\n",
    "print(f'Train: {len(x_train)} | Test: {len(x_test)}\\n')\n",
    "\n",
    "for idx, v in label_dict.items():\n",
    "    print(f'{v} | Train: {y_train.value_counts()[idx]} | Test: {y_test.value_counts()[idx]}')\n",
    "\n",
    "\"\"\" Set X(Data) and Y(Target) for external \"\"\"\n",
    "x_external = external_df.loc[:, X_features]\n",
    "y_external = external_df.loc[:, Y_target]\n",
    "print(f\"\\nExternal Feature: {x_external.shape} | Target: {y_external.shape}\")\n",
    "\n",
    "for idx, v in label_dict.items():\n",
    "    print(f'{v} | External: {y_external.value_counts()[idx]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd03bf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()/y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weighted-academy",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-header",
   "metadata": {},
   "source": [
    "- sklearn.base의 TransformerMixin을 상속하면 fit, transform메서드만 만들어도 자동으로 fit_transform()메서드를 자동으로 생성해준다.\n",
    "- sklearn.base의 BaseEstimator를 상속하면 하이퍼파라미터 튜닝에 필요한 두 메서드 get_params()와 set_params()를 추가로 얻게 된다. (생성자에 *args나 **kargs 사용하면 안됨)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-machinery",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSpliter( BaseEstimator, TransformerMixin ):\n",
    "    \"\"\" 변수 나누어주는 Class \n",
    "    수치형(Numerical) 데이터와 범주형(Categorical) 데이터를 선택하기 위한 클래스\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_names):\n",
    "        self.feature_names = feature_names\n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y = None):\n",
    "        return X[self.feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-tomato",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" 결측치 대체 Class \"\"\"\n",
    "class MissingTransformer( BaseEstimator, TransformerMixin ):\n",
    "    def __init__(self, imputer='single', strategy='mean'):\n",
    "        self.imputer = imputer\n",
    "        self.strategy = strategy\n",
    "\n",
    "    def fit( self, X, y = None ):\n",
    "        if (self.imputer == 'single') or (self.imputer == SimpleImputer()):\n",
    "            self.imputer = SimpleImputer(missing_values=np.nan, strategy=self.strategy)\n",
    "            if self.strategy not in ['mean', 'median', 'most_frequent', 'constant']:\n",
    "                raise ValueError(f\" >>> Can only use these strategies: ['mean', 'median', 'most_frequent', 'constant'], got strategy = {self.strategy}\")\n",
    "        elif self.imputer == 'multiple':\n",
    "            if (self.strategy == 'knn') or (self.imputer == KNNImputer()):\n",
    "                self.imputer = KNNImputer(missing_values=np.nan, n_neighbors=2)\n",
    "            elif (self.strategy == 'iterative') or (self.imputer == IterativeImputer()):\n",
    "                self.imputer = IterativeImputer(estimator=LinearRegression(), missing_values=np.nan, max_iter=10, verbose=2, \n",
    "                                                imputation_order='roman',random_state=random_state, n_nearest_features=5)\n",
    "            else: \n",
    "                raise ValueError(f\" >>> Can only use these strategies: ['knn', 'iterative'], got strategy = {self.strategy}\")\n",
    "                \n",
    "        self.imputer = self.imputer.fit(X)      \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X , y = None ):\n",
    "        # cols = X.columns.tolist()\n",
    "        result = self.imputer.transform(X)\n",
    "        # result = pd.DataFrame(result , columns=cols)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-diabetes",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalTransformer( BaseEstimator, TransformerMixin ):\n",
    "    \"\"\" 범주형 변수 처리 Class \"\"\"\n",
    "    def __init__(self, encoder ='ordinal'):\n",
    "        self.encoder = encoder\n",
    "        \n",
    "    def fit( self, X, y = None ):\n",
    "        if (self.encoder == 'ordinal') or (self.encoder == OrdinalEncoder()):\n",
    "            self.encoder = OrdinalEncoder()\n",
    "        # elif self.encoder == 'onehot':\n",
    "        #     self.encoder = OneHotEncoder()\n",
    "        # elif (self.encoder == 'label') or (self.encoder == LabelEncoder()):\n",
    "        #     self.encoder = LabelEncoder()\n",
    "        else:\n",
    "            raise ValueError(f\" >>> Can only use these encoders : ['ordinal'], got encoder = {self.encoder}\") # 'onehot', 'label'\n",
    "            \n",
    "        self.encoder = self.encoder.fit(X)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X , y = None ):\n",
    "        # cols = X.columns.to_list()\n",
    "        result = self.encoder.transform(X)\n",
    "        # result = pd.DataFrame(result, columns=cols)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-shelter",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericalTransformer( BaseEstimator, TransformerMixin ):\n",
    "    \"\"\" 연속형 변수 처리 Class \"\"\"\n",
    "    def __init__(self, scaler='minmax'):\n",
    "        self.scaler = scaler\n",
    "            \n",
    "    def fit( self, X, y = None ):\n",
    "        if (self.scaler == 'minmax') or (self.scaler == MinMaxScaler()):\n",
    "            self.scaler = MinMaxScaler()\n",
    "        elif (self.scaler == 'standard') or (self.scaler == StandardScaler()):\n",
    "            self.scaler = StandardScaler()\n",
    "        elif (self.scaler == 'normalize') or (self.scaler == Normalizer()):\n",
    "            self.scaler = Normalizer()\n",
    "        elif (self.scaler == 'robust') or (self.scaler == RobustScaler()):\n",
    "            self.scaler = RobustScaler()\n",
    "        else:\n",
    "            raise ValueError(f\" >>> Can only use these scalers : ['minmax', 'standard', 'normalize', 'robust'], got scaler = {self.scaler}\")\n",
    "            \n",
    "        self.scaler = self.scaler.fit(X)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X , y = None):\n",
    "        # cols = X.columns.to_list()\n",
    "        result = self.scaler.transform(X)\n",
    "        # result = pd.DataFrame(result, columns=cols)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historical-january",
   "metadata": {},
   "source": [
    "** Feature Selection **\n",
    "- 단일 변수 선택법은 각각의 독립변수를 하나만 사용한 예측모형의 성능을 이용하여 가장 분류성능 혹은 상관관계가 높은 변수만 선택하는 방법\n",
    "    - chi2: 카이제곱 검정 통계값\n",
    "    - f_classif: 분산분석(ANOVA) F검정 통계값\n",
    "    - mutual_info_classif: 상호정보량(mutual information)\n",
    "- feature_selection 서브패키지는 성능이 좋은 변수만 사용하는 전처리기인 SelectKBest 클래스도 제공\n",
    "- ** For regression: f_regression, mutual_info_regression **\n",
    "- ** For classification: chi2, f_classif, mutual_info_classif **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "federal-boost",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSelector( BaseEstimator, TransformerMixin ):\n",
    "    \"\"\" Feature Selection Class \"\"\"\n",
    "    def __init__(self, selector='chi2', k=10):\n",
    "        self.selector = selector\n",
    "        self.k = k\n",
    "            \n",
    "    def fit( self, X, y=None):\n",
    "        if (self.selector == 'chi2'): # filter method\n",
    "            score_func = chi2\n",
    "            self.selector = SelectKBest(score_func=score_func, k=self.k)\n",
    "        elif (self.selector == 'anova'): # filter method\n",
    "            score_func = f_classif\n",
    "            self.selector = SelectKBest(score_func=score_func, k=self.k)\n",
    "        elif (self.selector == 'mutualinfo'): # filter method\n",
    "            score_func = mutual_info_classif\n",
    "            self.selector = SelectKBest(score_func=score_func, k=self.k)\n",
    "        elif (self.selector == 'model-based'): # model-based, embedded method\n",
    "            model_sel = RandomForestClassifier(n_estimators=100, random_state=random_state).fit(X, y)\n",
    "            self.selector = SelectFromModel(model_sel, prefit=True, max_features=self.k)\n",
    "        else:\n",
    "            raise ValueError(f\" >>> Can only use these selectors : ['chi2', 'anova', 'mutualinfo','model-based'], got selector = {self.selector}\")\n",
    "        \n",
    "        self.selector = self.selector.fit(X, y)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X , y=None):\n",
    "        result = self.selector.transform(X)\n",
    "        return result\n",
    "    \n",
    "    def get_feature_name(self): # Get Selected Feature Name\n",
    "        return self.selector.get_feature_names_out(input_features=feature_col)\n",
    "    \n",
    "    def get_support(self):\n",
    "        return self.selector.get_support()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backed-strand",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiscal-prince",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "\n",
    "# def show_confusion_matrix(y_test, y_pred, save_path=None, dataset=None):\n",
    "#     conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "#     cmap = plt.cm.Blues\n",
    "#     plt.figure(figsize=(6,6))\n",
    "#     plt.imshow(conf_matrix, cmap=cmap, interpolation='nearest')\n",
    "\n",
    "#     thresh = conf_matrix.max() / 1.5 \n",
    "#     for i in range(conf_matrix.shape[0]):\n",
    "#         for j in range(conf_matrix.shape[1]):\n",
    "#             plt.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', color=\"white\" if conf_matrix[i, j] > thresh else \"black\",fontsize=fontsize) # , size='large'\n",
    "\n",
    "#     plt.xticks(np.arange(len(label_dict)), label_dict.values(), fontsize=fontsize, rotation=45)\n",
    "#     plt.yticks(np.arange(len(label_dict)), label_dict.values(), fontsize=fontsize, rotation=45)\n",
    "#     plt.xlabel('Predictions', fontsize=fontsize)\n",
    "#     plt.ylabel('Actuals', fontsize=fontsize)\n",
    "#     plt.title(f'Confusion matrix: {dataset}', fontsize=title_font)\n",
    "#     plt.colorbar(fraction=0.05, pad=0.05)\n",
    "#     if save_path:\n",
    "#         plt.savefig(f'{save_path}/Confusion_Matrix_{dataset}.png', bbox_inches='tight')\n",
    "#     plt.show()\n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8929ac6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion_matrix(y_test, y_pred, save_path=None, dataset=None):\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    conf_matrix = conf_matrix/conf_matrix.sum(1, keepdims=True) # np.divide(conf_matrix, conf_matrix.sum(1, keepdims=True))\n",
    "\n",
    "    cmap = plt.cm.Blues\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.imshow(conf_matrix, cmap=cmap, interpolation='nearest')\n",
    "\n",
    "    thresh = conf_matrix.max() / 1.5 \n",
    "    for i in range(conf_matrix.shape[0]):\n",
    "        for j in range(conf_matrix.shape[1]):\n",
    "            plt.text(x=j, y=i,s=round(conf_matrix[i, j], 2), va='center', ha='center', color=\"white\" if conf_matrix[i, j] > thresh else \"black\",fontsize=fontsize) # , size='large'\n",
    "\n",
    "    plt.xticks(np.arange(len(label_dict)), label_dict.values(), fontsize=fontsize, rotation=45)\n",
    "    plt.yticks(np.arange(len(label_dict)), label_dict.values(), fontsize=fontsize, rotation=45)\n",
    "    plt.xlabel('Predictions', fontsize=fontsize)\n",
    "    plt.ylabel('Actuals', fontsize=fontsize)\n",
    "    plt.title(f'Confusion matrix: {dataset}', fontsize=title_font)\n",
    "    plt.colorbar(fraction=0.05, pad=0.05)\n",
    "    if save_path:\n",
    "        plt.savefig(f'{save_path}/Confusion_Matrix_{dataset}.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-school",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.inspection import permutation_importance\n",
    "\n",
    "def show_permutation_importance(model, x, y, save_path=None, dataset=None):\n",
    "    \"\"\" \n",
    "    >>> PERMUTATION IMPORTANCE <<<  \n",
    "    > fix_model : 훈련된 모델\n",
    "    > X_train : 훈련데이터 Feature\n",
    "    > y_train : 훈련데이터 Target\n",
    "    > n_repeats : 특정 Feature 를 몇번 Shuffle 할 것인지\n",
    "    > scoring : Feature 를 Shuffler 한 뒤, 예측값과 실제값을 어떤 Metric 을 사용해 비교할지\n",
    "    > random_state : 난수 고정\n",
    "    \"\"\"\n",
    "    result = permutation_importance(model, x, y, n_repeats=10,\n",
    "                                    random_state=random_state, n_jobs=2)\n",
    "    \n",
    "    sorted_result = result.importances_mean.argsort()\n",
    "    # 결과를 DataFrame 화\n",
    "    importances = pd.DataFrame(result.importances_mean[sorted_result], index=feature_col[sorted_result]).sort_values(0, ascending=False)   \n",
    "    \n",
    "    # 결과를 시각화\n",
    "    plt.figure(figsize=(9,6))\n",
    "    top_index = sorted_result[-20:]\n",
    "    plt.boxplot(result.importances[top_index].T, vert=False, labels=feature_col[top_index])\n",
    "    plt.title(f\"Permutation Importances: TOP 20 ({dataset})\", fontsize=title_font)\n",
    "    plt.yticks(fontsize=fontsize)\n",
    "    plt.xticks(fontsize=fontsize)\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(f'{save_path}/Permutation_Importance_{dataset}.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-combining",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_roc_curve(model, x_test, y_test, save_path=None, dataset=None):\n",
    "    \"\"\" ROC & AUC \"\"\"\n",
    "    y_true = pd.get_dummies(y_test).to_numpy()\n",
    "    y_proba = model.predict_proba(x_test)\n",
    "\n",
    "    fprs = []\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))#, dpi=dpi)\n",
    "    mean_fpr = np.linspace(0,1,100)\n",
    "    for k, v in label_dict.items():\n",
    "        fpr, tpr, _ = roc_curve(y_true[:, k], y_proba[:, k])\n",
    "        auc_score = auc(fpr, tpr)\n",
    "        interp_tpr = np.interp(mean_fpr, fpr, tpr)\n",
    "        interp_tpr[0] = 0.0\n",
    "        tprs.append(interp_tpr)\n",
    "        aucs.append(auc_score)\n",
    "\n",
    "        line = ax.plot(fpr, tpr, label=f'{label_dict[k]} (AUC=%0.2f)'%auc_score, alpha=0.3)\n",
    "\n",
    "    ax.plot([0, 1], [0, 1], 'r--', label='Chance', alpha=0.8)\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    std_auc = np.std(aucs)\n",
    "    # print(\"Mean roc_auc_score: \", roc_auc_score(y_true, y_proba, multi_class='macro'))\n",
    "    ax.plot(mean_fpr, mean_tpr, color=mean_color, label='Mean ROC (AUC=%0.2f $\\pm$ %0.2f)'%(mean_auc, std_auc), alpha=0.8)\n",
    "\n",
    "\n",
    "    ax.legend(fontsize=small_font, ncol=1, loc='lower right')\n",
    "    plt.xlabel('False Positive Rate', fontsize=fontsize)\n",
    "    plt.ylabel('True Positive Rate', fontsize=fontsize)\n",
    "    plt.xticks(fontsize=fontsize)\n",
    "    plt.yticks(fontsize=fontsize)\n",
    "    plt.title(f'Receiver Operating Characteristic (ROC) Curve: {dataset}', fontsize=title_font)\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(f'{save_path}/ROC_Curve_{dataset}.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convertible-lawsuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_preprocessing(cat_feature=None, cat_encoder = 'ordinal', cat_imputer = 'single', cat_strategy = 'most_frequent', num_feature=None, num_scaler = 'minmax', num_imputer = 'single', num_strategy = 'mean'):\n",
    "    \"\"\" PipeLine: \n",
    "    >>> Feature Selector: Categorical or Numerical\n",
    "    >>> Imputer: Frequent or Mean\n",
    "    >>> Scaler: Normalization\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\" CREATE PREPROCESSING STEPS FOR CATEGORICAL PIPELINE \"\"\"\n",
    "    categorical_steps = [\n",
    "        ('cat_selector', FeatureSpliter(cat_feature)),\n",
    "        ('cat_encoder', CategoricalTransformer(encoder=cat_encoder)), \n",
    "        ('cat_imputer', MissingTransformer(imputer=cat_imputer, strategy=cat_strategy)),\n",
    "    ]\n",
    "\n",
    "    \"\"\" CREATE PREPROCESSING STEPS FOR NUMERICAL PIPELINE \"\"\"\n",
    "    numerical_steps = [\n",
    "        ('num_selector', FeatureSpliter(num_feature)),\n",
    "        ('num_imputer', MissingTransformer(imputer=num_imputer, strategy=num_strategy)),\n",
    "        ('num_scaler', NumericalTransformer(scaler=num_scaler)),\n",
    "    ]\n",
    "\n",
    "    # create the 2 pipelines with the respective steps\n",
    "    categorical_pipeline = Pipeline(categorical_steps)\n",
    "    numerical_pipeline = Pipeline(numerical_steps)\n",
    "\n",
    "    pipeline_list = [\n",
    "        ('categorical_pipeline', categorical_pipeline),\n",
    "        ('numerical_pipeline', numerical_pipeline)\n",
    "    ]\n",
    "    # Combining the 2 pieplines horizontally into one full pipeline\n",
    "    preprocessing_pipeline = FeatureUnion(transformer_list=pipeline_list)\n",
    "    \n",
    "    return preprocessing_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-brighton",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model(network):\n",
    "    if network == 'Multinomial':\n",
    "        model = LogisticRegression(multi_class='multinomial', solver='lbfgs', random_state = random_state)\n",
    "    elif network == 'Decision':\n",
    "        model = DecisionTreeClassifier(criterion='entropy', random_state=random_state)\n",
    "    elif network == 'SVC':\n",
    "        model = SVC(random_state = random_state)\n",
    "    elif  network == 'RF':\n",
    "        model = RandomForestClassifier(n_estimators = 1000, random_state = random_state)\n",
    "    elif network == 'SGD':\n",
    "        model = SGDClassifier(random_state = random_state)\n",
    "    elif network == 'XGB': # Good\n",
    "        model = XGBClassifier(random_state = random_state)\n",
    "    elif network == 'LGB': # Best\n",
    "        model = LGBMClassifier(random_state = random_state)\n",
    "    elif network == 'KNN': \n",
    "        model = KNeighborsClassifier(n_neighbors = 7)\n",
    "    elif network == 'Naive':\n",
    "        model = GaussianNB()\n",
    "    elif network == 'Gradient': # Good \n",
    "        model = GradientBoostingClassifier(random_state = random_state)\n",
    "    elif network == 'Cat': # Good \n",
    "        model = CatBoostClassifier(random_state = random_state)\n",
    "    elif network == 'MLP':\n",
    "        model = MLPClassifier(hidden_layer_sizes=(100,), activation='relu',solver='adam',batch_size=100,random_state=random_state)\n",
    "    # elif network == 'Logistic':\n",
    "    #     model = LogisticRegression(random_state = random_state)\n",
    "    else: \n",
    "        raise ValueError(f\" >>> Can only use these estimators : ['Multinomial', 'Decision', 'SVC', 'RF', 'SGD', 'XGB', 'LGB', 'KNN', 'Naive', 'Gradient', 'Cat', 'MLP'], got estimator = {network}\")\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0c151e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.metrics import euclidean_distances\n",
    "\n",
    "class EstimatorSelector( BaseEstimator, TransformerMixin ):\n",
    "    def __init__(self, estimator='Multinomial'):\n",
    "        self.estimator = estimator\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        if (self.estimator == 'Multinomial') or (self.estimator == LogisticRegression(multi_class='multinomial', solver='lbfgs', random_state = random_state)):\n",
    "            self.estimator = LogisticRegression(multi_class='multinomial', solver='lbfgs', random_state = random_state)\n",
    "        elif (self.estimator == 'Decision') or (self.estimator == DecisionTreeClassifier(criterion='entropy', random_state=random_state)):\n",
    "            self.estimator = DecisionTreeClassifier(criterion='entropy', random_state=random_state)\n",
    "        elif (self.estimator == 'SVC') or (self.estimator == SVC(random_state = random_state)):\n",
    "            self.estimator = SVC(random_state = random_state)\n",
    "        elif (self.estimator == 'RF') or (self.estimator == RandomForestClassifier(n_estimators = 1000, random_state = random_state)):\n",
    "            self.estimator = RandomForestClassifier(n_estimators = 1000, random_state = random_state)\n",
    "        elif (self.estimator == 'SGD') or (self.estimator == SGDClassifier(random_state = random_state)):\n",
    "            self.estimator = SGDClassifier(random_state = random_state)\n",
    "        elif (self.estimator == 'XGB') or (self.estimator == XGBClassifier(random_state = random_state)): # Good\n",
    "            self.estimator = XGBClassifier(random_state = random_state)\n",
    "        elif (self.estimator == 'LGB') or (self.estimator == LGBMClassifier(random_state = random_state)): # Best\n",
    "            self.estimator = LGBMClassifier(random_state = random_state)\n",
    "        elif (self.estimator == 'KNN') or (self.estimator == KNeighborsClassifier(n_neighbors = 7)): \n",
    "            self.estimator = KNeighborsClassifier(n_neighbors = 7)\n",
    "        elif (self.estimator == 'Naive') or (self.estimator == GaussianNB()):\n",
    "            self.estimator = GaussianNB()\n",
    "        elif (self.estimator == 'Gradient') or (self.estimator == GradientBoostingClassifier(random_state = random_state)): # Good \n",
    "            self.estimator = GradientBoostingClassifier(random_state = random_state)\n",
    "        elif (self.estimator == 'Cat') or (self.estimator == CatBoostClassifier(random_state = random_state)): # Good \n",
    "            self.estimator = CatBoostClassifier(random_state = random_state)\n",
    "        elif (self.estimator == 'MLP') or (self.estimator == MLPClassifier(hidden_layer_sizes=(100,), activation='relu',solver='adam',batch_size=100,random_state=random_state)):\n",
    "            self.estimator = MLPClassifier(hidden_layer_sizes=(100,), activation='relu',solver='adam',batch_size=100,random_state=random_state)\n",
    "        # elif (self.estimator == 'Logistic') or (self.estimator == LogisticRegression(random_state = random_state)):\n",
    "        #     self.estimator = LogisticRegression(random_state = random_state)\n",
    "        else: \n",
    "            raise ValueError(f\" >>> Can only use these estimators : ['Multinomial', 'Decision', 'SVC', 'RF', 'SGD', 'XGB', 'LGB', 'KNN', 'Naive', 'Gradient', 'Cat', 'MLP'], got estimator = {self.estimator}\")\n",
    "            \n",
    "        self.estimator.fit(X,y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = self.estimator.predict(X)\n",
    "        return y_pred\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        score = self.estimator.score(X, y)\n",
    "        return score\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        proba = self.estimator.predict_proba(X)\n",
    "        return proba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-luxury",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "framed-mileage",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" SELECT VARIABLES AND CREATE PIPELINE \"\"\"\n",
    "cat_encoder = 'ordinal'\n",
    "cat_imputer = 'single' # 'single' 'multiple'\n",
    "cat_strategy = 'most_frequent' # 'most_frequent' 'knn'\n",
    "num_scaler = 'robust' # 'minmax' 'standard' 'normalize' 'robust'\n",
    "num_imputer = 'single' # 'single' 'multiple'\n",
    "num_strategy = 'mean' # 'mean' 'iterative'\n",
    "preprocessing_pipe = set_preprocessing(categorical_features, cat_encoder, cat_imputer, cat_strategy, numerical_features, num_scaler, num_imputer, num_strategy)\n",
    "\n",
    "select_func = 'mutualinfo' # 'chi2', 'anova', 'mutualinfo'\n",
    "select_k = 35\n",
    "selector_pipe = FeatureSelector(selector=select_func, k=select_k)\n",
    "\n",
    "# Feature Extraction : 피쳐들을 선택하는 것이 아니라, 더 작은 차원으로 피쳐들을 맵핑하는 것이다. \n",
    "# PCA, LDA, SVD, NMF\n",
    "# n_components = 40\n",
    "# pca_pipe = PCA(n_components=n_components, random_state\n",
    "# =random_state)\n",
    "\n",
    "network = 'LGB'\n",
    "# model_pipe = set_model(network)\n",
    "model_pipe  = EstimatorSelector(estimator=network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-middle",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set model pipeline\n",
    "pipeline = Pipeline([('Processing', preprocessing_pipe),\n",
    "                  # ('Selector', selector_pipe),\n",
    "                #   ('PCA', pca_pipe),\n",
    "                  ('Model', model_pipe),\n",
    "                 ])\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "still-organization",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" Model Train \"\"\"\n",
    "pipeline.fit(x_train, y_train)\n",
    "y_pred = pipeline.predict(x_test)\n",
    "y_pred_external = pipeline.predict(x_external)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03a8cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\">>> Test Accuracy / Weighted F1-Score: {:.4f} / {:.4f}\".format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred, average='weighted')))\n",
    "print(\">>> External Accuracy / Weighted F1-Score: {:.4f} / {:.4f}\".format(accuracy_score(y_external, y_pred_external), f1_score(y_external, y_pred_external, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081462b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 실험한 모델들을 저장 \"\"\"\n",
    "# SAVE PATH 지정\n",
    "# SAVE_RESULT_PATH = f'{data_dir}/{len(X_features)}/{network}/{num_imputer}_{num_scaler}'\n",
    "SAVE_RESULT_PATH = f'{data_dir}/{test_type}/{network}/{num_imputer}_{num_scaler}'\n",
    "os.makedirs(SAVE_RESULT_PATH, exist_ok=True)\n",
    "joblib.dump(pipeline, os.path.join(SAVE_RESULT_PATH, f\"best_model.pkl\"))\n",
    "# m_load = joblib.load(os.path.join(SAVE_RESULT_PATH, f\"model_{network}.pkl\"))\n",
    "# print(\">>> Test Accuracy / Weighted F1-Score: {:.2f} / {:.2f}\".format(m_load.score(x_test, y_test), f1_score(y_test, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "express-inclusion",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'>>> Current Model: {network} <<<')\n",
    "print(f'>>> Current Scaler: {num_scaler} <<<')\n",
    "print(f'>>> Current Imputer: {num_imputer} <<<')\n",
    "print(f'Number of Train Dataset: {len(x_train)}')\n",
    "print(f'Number of Test Dataset: {len(x_test)}')\n",
    "print(\">>> Train Accuracy / Weighted F1-Score: {:.4f} / {:.4f}\".format(pipeline.score(x_train, y_train), f1_score(y_train, pipeline.predict(x_train), average='weighted')))\n",
    "print(\">>> Test Accuracy / Weighted F1-Score: {:.4f} / {:.4f}\".format(pipeline.score(x_test, y_test), f1_score(y_test, y_pred, average='weighted')))\n",
    "print(classification_report(y_test, y_pred, target_names=list(label_dict.values())))\n",
    "\n",
    "print(\">>> External Accuracy / Weighted F1-Score: {:.4f} / {:.4f}\".format(pipeline.score(x_external, y_external), f1_score(y_external, y_pred_external, average='weighted')))\n",
    "print(classification_report(y_external, y_pred_external, target_names=list(label_dict.values())))\n",
    "\n",
    "# # View Selected Features\n",
    "# print('-'*30)\n",
    "# print(pipeline.named_steps['Selector'].get_feature_name())\n",
    "# print(\"Number of Selected Feature: {}\".format(len(pipeline.named_steps['Selector'].get_feature_name())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-injury",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Model Results Save \"\"\"\n",
    "results_dict = dict()\n",
    "results_dict['Model'] = network\n",
    "results_dict['Scaler'] = num_scaler\n",
    "results_dict['Imputer'] = num_imputer\n",
    "results_dict['<<< TRAIN >>>'] = ''\n",
    "results_dict['Train Set'] = len(x_train)\n",
    "results_dict['Train Accuracy'] = round(pipeline.score(x_train, y_train), 4)\n",
    "results_dict['Train Weighted F1-Score'] = round(f1_score(y_train, pipeline.predict(x_train), average='weighted'), 4)\n",
    "results_dict['<<< TEST >>>'] = ''\n",
    "results_dict['Test Set'] = len(x_test)\n",
    "results_dict['Test Accuracy'] = round(pipeline.score(x_test, y_test), 4)\n",
    "results_dict['Test Weighted F1-Score'] = round(f1_score(y_test, y_pred, average='weighted'), 4)\n",
    "results_dict['Results Table: Test'] = f'\\n{classification_report(y_test, y_pred, target_names=list(label_dict.values()))}'\n",
    "results_dict['<<< EXTERNAL >>>'] = ''\n",
    "results_dict['External Set'] = len(x_external)\n",
    "results_dict['External Accuracy'] = round(pipeline.score(x_external, y_external), 4)\n",
    "results_dict['External Weighted F1-Score'] = round(f1_score(y_external, y_pred_external, average='weighted'), 4)\n",
    "results_dict['Results Table: External'] = f'\\n{classification_report(y_external, y_pred_external, target_names=list(label_dict.values()))}'\n",
    "# results_dict['Selected Features'] = pipeline.named_steps['Selector'].get_feature_name()\n",
    "# results_dict['Number of Selected Features'] = len(pipeline.named_steps['Selector'].get_feature_name())\n",
    "\n",
    "f = open(os.path.join(SAVE_RESULT_PATH, \" results_info.txt\"), \"w\")\n",
    "f.write(f' --- Results --- \\n')\n",
    "for k, v in results_dict.items():\n",
    "    f.write(f'[{k}]: {v}\\n')\n",
    "f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-adventure",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_confusion_matrix(y_test, y_pred, save_path=SAVE_RESULT_PATH, dataset='Test')\n",
    "show_confusion_matrix(y_external, y_pred_external, save_path=SAVE_RESULT_PATH, dataset='External')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7ccb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_roc_curve(pipeline, x_test, y_test, save_path=SAVE_RESULT_PATH, dataset='Test')\n",
    "show_roc_curve(pipeline, x_external, y_external, save_path=SAVE_RESULT_PATH, dataset='External')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive-repeat",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_permutation_importance(pipeline, x_test, y_test, save_path=SAVE_RESULT_PATH, dataset='Test')\n",
    "show_permutation_importance(pipeline, x_external, y_external, save_path=SAVE_RESULT_PATH, dataset='External')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe63560",
   "metadata": {},
   "source": [
    "# Wrapper Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297aa314",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" SELECT VARIABLES AND CREATE PIPELINE \"\"\"\n",
    "cat_encoder = 'ordinal'\n",
    "cat_imputer = 'single' # 'single' 'multiple'\n",
    "cat_strategy = 'most_frequent' # 'most_frequent' 'knn'\n",
    "num_scaler = 'robust' # 'minmax' 'standard' 'normalize' 'robust'\n",
    "num_imputer = 'single' # 'single' 'multiple'\n",
    "num_strategy = 'mean' # 'mean' 'iterative'\n",
    "preprocessing_pipe = set_preprocessing(categorical_features, cat_encoder, cat_imputer, cat_strategy, numerical_features, num_scaler, num_imputer, num_strategy)\n",
    "\n",
    "network = 'LGB'\n",
    "model_pipe = set_model(network)\n",
    "# model_pipe  = EstimatorSelector(estimator=network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a68c41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_pipe.fit(x_train)\n",
    "x_train_trans = preprocessing_pipe.transform(x_train)\n",
    "x_test_trans = preprocessing_pipe.transform(x_test)\n",
    "x_external_trans = preprocessing_pipe.transform(x_external)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5a8daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\" Model Train \"\"\"\n",
    "# step = 1\n",
    "# min_features_to_select = 5\n",
    "# rfe_selector = RFE(estimator=model_pipe, n_features_to_select=min_features_to_select, step=step, verbose=5)\n",
    "# rfe_selector.fit(x_train_trans, y_train)\n",
    "# y_pred = rfe_selector.predict(x_test_trans)\n",
    "# y_pred_external = rfe_selector.predict(x_external_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b014d260",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "cv = 5 #RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=random_state)\n",
    "step = 1\n",
    "min_features_to_select = 5\n",
    "rfe_selector = RFECV(estimator=model_pipe, step=step, scoring='f1_weighted', min_features_to_select=min_features_to_select, cv=cv)\n",
    "rfe_selector.fit(x_train_trans, y_train)\n",
    "y_pred = rfe_selector.predict(x_test_trans)\n",
    "\n",
    "y_pred_external = rfe_selector.predict(x_external_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809c4c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe_selector.support_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32080695",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Performance per number of features\")\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (f1_weighted)\")\n",
    "plt.plot(\n",
    "    range(min_features_to_select, len(rfe_selector.grid_scores_)*step + min_features_to_select, step),\n",
    "    rfe_selector.grid_scores_, '--',\n",
    "    alpha=0.5,\n",
    ")\n",
    "plt.plot(\n",
    "    range(min_features_to_select, len(rfe_selector.grid_scores_)*step + min_features_to_select, step),\n",
    "    np.mean(rfe_selector.grid_scores_, axis=1), color='red', \n",
    "    label='Mean',\n",
    ")\n",
    "plt.scatter(np.argmax(np.mean(rfe_selector.grid_scores_, axis=1))+min_features_to_select, np.max(np.mean(rfe_selector.grid_scores_, axis=1)),marker='*', color='red',s=100)\n",
    "plt.legend()\n",
    "# if save_path:\n",
    "#     plt.savefig(f'{save_path}/ROC_Curve_{dataset}.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21811067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rfe_selector.estimator_\n",
    "# rfe_selector.n_features_\n",
    "# rfe_selector.support_\n",
    "# rfe_selector.ranking_\n",
    "# rfe_selector.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35801aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe_selector.ranking_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bd5a27",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'>>> Current Model: {network} <<<')\n",
    "print(f'>>> Current Scaler: {num_scaler} <<<')\n",
    "print(f'>>> Current Imputer: {num_imputer} <<<')\n",
    "print(f'Number of Train Dataset: {len(x_train_trans)}')\n",
    "print(f'Number of Test Dataset: {len(x_test_trans)}')\n",
    "print(\">>> Train Accuracy / Weighted F1-Score: {:.4f} / {:.4f}\".format(rfe_selector.score(x_train_trans, y_train), f1_score(y_train, rfe_selector.predict(x_train_trans), average='weighted')))\n",
    "print(\">>> Test Accuracy / Weighted F1-Score: {:.4f} / {:.4f}\".format(rfe_selector.score(x_test_trans, y_test), f1_score(y_test, y_pred, average='weighted')))\n",
    "print(classification_report(y_test, y_pred, target_names=list(label_dict.values())))\n",
    "print(\">>> External Accuracy / Weighted F1-Score: {:.4f} / {:.4f}\".format(rfe_selector.score(x_external_trans, y_external), f1_score(y_external, y_pred_external, average='weighted')))\n",
    "print(classification_report(y_external, y_pred_external, target_names=list(label_dict.values())))\n",
    "\n",
    "rfe_support = rfe_selector.get_support()\n",
    "rfe_feature = x_train.loc[:,rfe_support].columns.tolist()\n",
    "print(rfe_feature)\n",
    "print(f'Number of Features: {len(rfe_feature)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47caf0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 실험한 모델들을 저장 \"\"\"\n",
    "# SAVE PATH 지정 \n",
    "SAVE_RESULT_PATH = f'{data_dir}/{len(X_features)}/RFECV/{network}/{num_imputer}_{num_scaler}'\n",
    "# SAVE_RESULT_PATH = f'results/Test'\n",
    "os.makedirs(SAVE_RESULT_PATH, exist_ok=True)\n",
    "joblib.dump(rfe_selector, os.path.join(SAVE_RESULT_PATH, f\"best_model.pkl\"))\n",
    "# m_load = joblib.load(os.path.join(SAVE_RESULT_PATH, f\"model_{network}.pkl\"))\n",
    "# print(\">>> Test Accuracy / Weighted F1-Score: {:.2f} / {:.2f}\".format(m_load.score(x_test, y_test), f1_score(y_test, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5b2a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Model Results Save \"\"\"\n",
    "results_dict = dict()\n",
    "results_dict['Model'] = network\n",
    "results_dict['Scaler'] = num_scaler\n",
    "results_dict['Imputer'] = num_imputer\n",
    "results_dict['<<< TRAIN >>>'] = ''\n",
    "results_dict['Train Set'] = len(x_train_trans)\n",
    "results_dict['Train Accuracy'] = round(rfe_selector.score(x_train_trans, y_train), 4)\n",
    "results_dict['Train Weighted F1-Score'] = round(f1_score(y_train, rfe_selector.predict(x_train_trans), average='weighted'), 4)\n",
    "results_dict['<<< TEST >>>'] = ''\n",
    "results_dict['Test Set'] = len(x_test_trans)\n",
    "results_dict['Test Accuracy'] = round(rfe_selector.score(x_test_trans, y_test), 4)\n",
    "results_dict['Test Weighted F1-Score'] = round(f1_score(y_test, y_pred, average='weighted'), 4)\n",
    "results_dict['Results Table: Test'] = f'\\n{classification_report(y_test, y_pred, target_names=list(label_dict.values()))}'\n",
    "results_dict['<<< EXTERNAL >>>'] = ''\n",
    "results_dict['External Set'] = len(x_external_trans)\n",
    "results_dict['External Accuracy'] = round(rfe_selector.score(x_external_trans, y_external), 4)\n",
    "results_dict['External Weighted F1-Score'] = round(f1_score(y_external, y_pred_external, average='weighted'), 4)\n",
    "results_dict['Results Table: External'] = f'\\n{classification_report(y_external, y_pred_external, target_names=list(label_dict.values()))}'\n",
    "results_dict['<<< SELECTED FEATURE >>>'] = ''\n",
    "results_dict['Selected Features'] = rfe_feature\n",
    "results_dict['Number of Selected Features'] = len(rfe_feature)\n",
    "\n",
    "f = open(os.path.join(SAVE_RESULT_PATH, \" results_info.txt\"), \"w\")\n",
    "f.write(f' --- Results --- \\n')\n",
    "for k, v in results_dict.items():\n",
    "    f.write(f'[{k}]: {v}\\n')\n",
    "f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad854d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_confusion_matrix(y_test, y_pred, save_path=SAVE_RESULT_PATH, dataset='Test')\n",
    "show_confusion_matrix(y_external, y_pred_external, save_path=SAVE_RESULT_PATH, dataset='External')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a606bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_permutation_importance(rfe_selector, x_test_trans, y_test, save_path=SAVE_RESULT_PATH, dataset='Test')\n",
    "# show_permutation_importance(rfe_selector, x_external, y_external, save_path=SAVE_RESULT_PATH, dataset='External')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3985b448",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_roc_curve(rfe_selector, x_test_trans, y_test, save_path=SAVE_RESULT_PATH, dataset='Test')\n",
    "show_roc_curve(rfe_selector, x_external_trans, y_external, save_path=SAVE_RESULT_PATH, dataset='External')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc57bc71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "republican-seventh",
   "metadata": {},
   "source": [
    "# GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a97c691",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" SELECT VARIABLES AND CREATE PIPELINE \"\"\"\n",
    "cat_encoder = 'ordinal'\n",
    "cat_imputer = 'single' # 'single' 'multiple'\n",
    "cat_strategy = 'most_frequent' # 'most_frequent' 'knn'\n",
    "num_scaler = 'standard' # 'minmax' 'standard' 'normalize' 'robust'\n",
    "num_imputer = 'single' # 'single' 'multiple'\n",
    "num_strategy = 'mean' # 'mean' 'iterative'\n",
    "preprocessing_pipe = set_preprocessing(categorical_features, cat_encoder, cat_imputer, cat_strategy, numerical_features, num_scaler, num_imputer, num_strategy)\n",
    "\n",
    "select_func = 'mutualinfo' # 'chi2', 'anova', 'mutualinfo'\n",
    "select_k = 49\n",
    "selector_pipe = FeatureSelector(selector=select_func, k=select_k)\n",
    "\n",
    "# Feature Extraction : 피쳐들을 선택하는 것이 아니라, 더 작은 차원으로 피쳐들을 맵핑하는 것이다. \n",
    "# PCA, LDA, SVD, NMF\n",
    "# n_components = 40\n",
    "# pca_pipe = PCA(n_components=n_components, random_state\n",
    "# =random_state)\n",
    "\n",
    "network = 'LGB'\n",
    "# model_pipe = set_model(network)\n",
    "model_pipe  = EstimatorSelector(estimator=network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-donna",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model pipeline\n",
    "# steps=[('Processing', preprocessing_pipe), ('Selector', selector_pipe), ('PCA', pca_pipe), ('Model', model_pipe)]\n",
    "\n",
    "steps=[('Processing', preprocessing_pipe), ('Selector', selector_pipe), ('Model', model_pipe)]\n",
    "\n",
    "pipeline_grid = Pipeline(steps=steps)\n",
    "pipeline_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2831605f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_grid.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-milan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline_grid.get_params().keys()\n",
    "\n",
    "params = {\n",
    "    # 'Processing__numerical_pipeline__num_scaler__scaler': ['minmax','standard','robust'],\n",
    "    # 'Processing__numerical_pipeline__num_imputer__':[],\n",
    "    'Selector__k':[3, 5, 10, 12, 15, 18, 20, 21,22,23,24,25], # [3, 4, 5,6,7,8,9, 10,11, 12,13,14, 15], # \n",
    "    # 'Selector__selector':['chi2','anova','mutualinfo'], # 'chi2',-> input X must be non-negative\n",
    "    # 'PCA__n_components': [min(5, ), 12],\n",
    "    # 'Model__estimator':['Multinomial', 'Decision', 'SVC', 'RF', 'SGD', 'XGB', 'LGB', 'KNN', 'Naive', 'Gradient', 'Cat'],\n",
    "    # 'Model__C':[0.1,1, 10, 100],\n",
    "    # 'Model__gamma':['scale','auto'], #[1,0.1,0.01,0.001],\n",
    "    # 'Model__kernel':['linear','rbf', 'poly', 'sigmoid'],\n",
    "    # ### MLP\n",
    "    # # 'Model__activation':['identity','logistic','tanh','relu'],\n",
    "    # 'Model__alpha':[0.0001,0.001,0.01,0.1],\n",
    "    # 'Model__hidden_layer_sizes':[(2,),(5,),(10,)], # ,(100,)\n",
    "    # # 'Model__solver':['lbfgs','sgd','adam'],\n",
    "    # # 'Model__learning_rate':['constant','invscaling','adaptive'],\n",
    "    # 'Model__learning_rate_init':[0.0001,0.001, 0.01, 0.1],\n",
    "    # 'Model__max_iter':[25, 50, 75, 100],\n",
    "    # 'Model__momentum':[0.1,0.3,0.5,0.9],\n",
    "    # ### LGB\n",
    "    # 'Model__learning_rate':[0.01, 0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    # 'Model__max_depth':[25, 50, 75],\n",
    "    # 'Model__num_leaves':[100,300,500,900,1200],\n",
    "    # 'Model__n_estimators':[100,200,300,500,800,1000],\n",
    "    # ### RF\n",
    "    # 'Model__n_estimators':[100,200,300,500],\n",
    "    # 'Model__max_depth':[25, 50, 75],\n",
    "    # 'Model__max_leaf_nodes':[25, 50, 100],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "after-effort",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn.metrics.get_scorer_names()\n",
    "\n",
    "# StratifiedKFold 라벨의 비율을 유지하며 교차검증\n",
    "kf = 5 #StratifiedKFold(n_splits=5)\n",
    "\n",
    "grid_search = GridSearchCV(pipeline_grid, param_grid=params,scoring='f1_weighted',n_jobs=-1, cv=kf, refit=True, verbose=2, error_score='raise')\n",
    "grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ethical-geography",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-ethernet",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('best parameters: ', grid_search.best_params_)\n",
    "# print('best estimator: ', grid_search.best_estimator_)\n",
    "print('best score: ', grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latest-checkout",
   "metadata": {},
   "outputs": [],
   "source": [
    "em = grid_search.best_estimator_\n",
    "y_pred = em.predict(x_test)\n",
    "y_pred_external = em.predict(x_external)\n",
    "\n",
    "print(\">>> Train Accuracy / Weighted F1-Score: {:.4f} / {:.4f}\".format(em.score(x_train, y_train), f1_score(y_train, em.predict(x_train), average='weighted')))\n",
    "print(\">>> Test Accuracy / Weighted F1-Score: {:.4f} / {:.4f}\".format(em.score(x_test, y_test), f1_score(y_test, em.predict(x_test), average='weighted')))\n",
    "print(\">>> External Accuracy / Weighted F1-Score: {:.4f} / {:.4f}\".format(em.score(x_external, y_external), f1_score(y_external, y_pred_external, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26781c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "em.named_steps['Selector'].get_feature_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-gibson",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=list(label_dict.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interesting-organization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE PATH 지정 \n",
    "SAVE_RESULT_PATH = f\"{data_dir}/BestGridCV/{network}/{select_func}/{num_imputer}_{num_scaler}_{grid_search.best_params_['Selector__k']}\"\n",
    "# SAVE_RESULT_PATH = f'results/Test'\n",
    "os.makedirs(SAVE_RESULT_PATH, exist_ok=True)\n",
    "joblib.dump(em, os.path.join(SAVE_RESULT_PATH, f\"best_model.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f3f923",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Model Results Save \"\"\"\n",
    "results_dict = dict()\n",
    "results_dict['Model'] = network\n",
    "results_dict['Scaler'] = num_scaler\n",
    "results_dict['Imputer'] = num_imputer\n",
    "results_dict['Train Set'] = len(x_train)\n",
    "results_dict['Train Accuracy'] = round(em.score(x_train, y_train), 4)\n",
    "results_dict['Train Weighted F1-Score'] = round(f1_score(y_train, em.predict(x_train), average='weighted'), 4)\n",
    "results_dict['Test Set'] = len(x_test)\n",
    "results_dict['Test Accuracy'] = round(em.score(x_test, y_test), 4)\n",
    "results_dict['Test Weighted F1-Score'] = round(f1_score(y_test, y_pred, average='weighted'), 4)\n",
    "results_dict['Results Table: Test'] = f'\\n{classification_report(y_test, y_pred, target_names=list(label_dict.values()))}'\n",
    "results_dict['External Set'] = len(x_external)\n",
    "results_dict['External Accuracy'] = round(em.score(x_external, y_external), 4)\n",
    "results_dict['External Weighted F1-Score'] = round(f1_score(y_external, y_pred_external, average='weighted'), 4)\n",
    "results_dict['Results Table: External'] = f'\\n{classification_report(y_external, y_pred_external, target_names=list(label_dict.values()))}'\n",
    "results_dict['Best Parameters'] = grid_search.best_params_\n",
    "results_dict['Selected Features'] = em.named_steps['Selector'].get_feature_name()\n",
    "results_dict['Number of Selected Features'] = len(em.named_steps['Selector'].get_feature_name())\n",
    "\n",
    "f = open(os.path.join(SAVE_RESULT_PATH, \" results_info.txt\"), \"w\")\n",
    "f.write(f' --- Results --- \\n')\n",
    "for k, v in results_dict.items():\n",
    "    f.write(f'[{k}]: {v}\\n')\n",
    "f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-fifteen",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_confusion_matrix(y_test, y_pred, save_path=SAVE_RESULT_PATH, dataset='Test')\n",
    "show_confusion_matrix(y_external, y_pred_external, save_path=SAVE_RESULT_PATH, dataset='External')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de33c313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_permutation_importance(em, x_test, y_test, save_path=SAVE_RESULT_PATH, dataset='Test') # \n",
    "# show_permutation_importance(em, x_external, y_external, save_path=SAVE_RESULT_PATH, dataset='External') # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "packed-insurance",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_roc_curve(em, x_test, y_test, save_path=SAVE_RESULT_PATH, dataset='Test')\n",
    "show_roc_curve(em, x_external, y_external, save_path=SAVE_RESULT_PATH, dataset='External')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f358dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d841de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb654a32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Machine Learning",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "3b71506ace91668b7e3715d77003e0ced71cc3f585a9c2fe78e596645c7df6d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
